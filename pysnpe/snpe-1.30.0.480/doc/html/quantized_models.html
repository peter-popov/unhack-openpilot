<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Quantized vs Non-Quantized Models</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('quantized_models.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Quantized vs Non-Quantized Models </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="overview_quantize"></a>
Overview</h1>
<ul>
<li>
Non-quantized DLC files use 32 bit floating point representations of network parameters. </li>
<li>
Quantized DLC files use 8 bit fixed point representations of network parameters. The fixed point representation is the same used in Tensorflow quantized models. </li>
</ul>
<h1><a class="anchor" id="caffe_quantize"></a>
Caffe and Caffe2</h1>
<p>The default output of <a class="el" href="tools.html#tools_snpe-caffe-to-dlc">snpe-caffe-to-dlc</a> and <a class="el" href="tools.html#tools_snpe-caffe2-to-dlc">snpe-caffe2-to-dlc</a> is a non-quantized model. This means that all the network parameters are left in the 32 floating point representation as present in the original Caffe model. To quantize the model to 8 bit fixed point, see <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>. Note that models that are intended to be quantized using <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a> must have their batch dimension set to 1. A different batch dimension can be used during inference, by <a class="el" href="network_resize.html">resizing</a> the network during initialization.</p>
<h1><a class="anchor" id="onnx_quantize"></a>
ONNX</h1>
<p>The default output of <a class="el" href="tools.html#tools_snpe-onnx-to-dlc">snpe-onnx-to-dlc</a> is a non-quantized model. This means that all the network parameters are left in the 32 bit floating point representation as present in the original ONNX model. To quantize the model to 8 bit fixed point, see <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>. Note that models that are intended to be quantized using <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a> must have their batch dimension set to 1. A different batch dimension can be used during inference, by <a class="el" href="network_resize.html">resizing</a> the network during initialization.</p>
<h1><a class="anchor" id="tensorflow_quantize"></a>
TensorFlow</h1>
<p>The default output of <a class="el" href="tools.html#tools_snpe-tensorflow-to-dlc">snpe-tensorflow-to-dlc</a> is a non-quantized model. This means that all the network parameters are left in the 32 bit floating point representation as present in the original TensorFlow model. To quantize the model to 8 bit fixed point, see <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>. Note that models that are intended to be quantized using <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a> must have their batch dimension set to 1. A different batch dimension can be used during inference, by <a class="el" href="network_resize.html">resizing</a> the network during initialization.</p>
<p>The TensorFlow converter does not support conversion of TensorFlow graphs that have been quantized using TensorFlow tools.</p>
<h1><a class="anchor" id="quantize_or_not"></a>
Choosing Between a Quantized or Non-Quantized Model</h1>
<h2><a class="anchor" id="quantize_or_not_summary"></a>
Summary</h2>
<p> <p><table class="doxtable" width="100%">  <tr>  <th colspan="1">Runtime</th> <th colspan="1">Quantized DLC</th> <th colspan="1">Non-Quantized DLC</th>  </tr>  <tr> <td> CPU or GPU </td>  <td> <b>Compatible.</b> The model is dequantized by the runtime, increasing network initialization time. Accuracy may be impacted. </td>  <td> <b>Compatible.</b> The model is native format for these runtimes. Model can be passed directly to the runtime. May be more accurate than a quantized model. </td>  <tr> <td> DSP </td>  <td> <b>Compatible.</b> The model is native format for DSP runtime. Model can be passed directly to the runtime. Accuracy may be different than a non-quantized model </td>  <td> <b>Compatible.</b> The model is quantized by the runtime, increasing network initialization time. Accuracy may be different than a quantized model. </td>  <tr> <td> AIP </td>  <td> <b>Compatible.</b> The model is in supported format for AIP runtime. Model can be passed directly to the runtime. </td>  <td> <b>Incompatible.</b> Non-quantized models are not supported by the AIP runtime. </td>  </table></p> <h2><a class="anchor" id="quantize_or_not_details"></a>
Details</h2>
<ul>
<li>
GPU and CPU <ul>
<li>
The GPU and CPU always use floating point (non-quantized) network parameters. </li>
<li>
Using quantized DLC files with GPU and CPU runtimes is supported. Network initialization time will dramatically increase as SNPE will automatically de-quantize the network parameters in order to run on GPU and CPU. </li>
<li>
If network initialization time is a concern, it is recommended to use non-quantized DLC files (default) for both GPU and CPU. </li>
<li>
Quantization of the DLC file does introduce noise, as quantization is lossy. </li>
<li>
The network performance during execute is not impacted by the choice of quantized vs non-quantized DLC files. </li>
</ul>
</li>
<li>
DSP <ul>
<li>
The DSP always uses quantized network parameters. </li>
<li>
Using a non-quantized DLC file on the DSP is supported. Network initialization time will dramatically increase as SNPE will automatically quantize the network parameters in order to run on the DSP. </li>
<li>
It is generally recommended to use quantized DLC files for running on the DSP. In addition to faster network initialization time, using quantized models also reduces peak memory usage during initialization, and decreases DLC file size. </li>
</ul>
</li>
<li>
AIP <ul>
<li>
The AIP runtime always uses quantized network parameters. </li>
<li>
Passing through snpe-dlc-quantize is mandatory for generating the binaries for HTA subnets.  </li>
<li>
Using a non-quantized DLC file with the AIP runtime is not supported. </li>
<li>
HTA subnets use the quantized parameters in the DLC. </li>
<li>
HNN (Hexagon NN) subnets use the quantization parameters in the same way DSP runtime does. </li>
</ul>
</li>
<li>
Balancing DLC file size, network initialization time and accuracy <ul>
<li>
If the network will mainly run on the GPU and CPU it is recommended to try both quantized and non-quantized models during development. If a quantized model provides enough accuracy, then it may be used at the expense of increased network initialization time. The benefit is a much smaller DLC file. The tradeoff between accuracy, network initialization time, and DLC file size is application specific. </li>
<li>
If the network will mainly run on the DSP, there is no benefit to using a non-quantized model. As previously stated it will dramatically increase network initialization time and DLC file size, but provide no accuracy benefit. </li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="quantize_algorithm"></a>
Quantization Algorithm</h1>
<p>This section describes the concepts behind the quantization algorithm used in SNPE. These concepts are used by <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a> and is also used by SNPE for input quantization when using the DSP runtime.</p>
<h2><a class="anchor" id="quantize_algorithm_overview"></a>
Overview</h2>
<p><b>Note:</b> SNPE supports multiple quantization modes. The basics of the quantization, regardless of mode, are described here. See <a class="el" href="quantized_models.html#quantization_modes">Quantization Modes</a> for more information. </p><ul>
<li>
Quantization converts floating point data to Tensorflow-style 8-bit fixed point format </li>
<li>
The following requirements are satisfied: <ul>
<li>
Full range of input values is covered. </li>
<li>
Minimum range of 0.01 is enforced. </li>
<li>
Floating point zero is exactly representable. </li>
</ul>
</li>
<li>
Quantization algorithm inputs: <ul>
<li>
Set of floating point values to be quantized. </li>
</ul>
</li>
<li>
Quantization algorithm outputs: <ul>
<li>
Set of 8-bit fixed point values. </li>
<li>
Encoding parameters: <ul>
<li>
encoding-min - minimum floating point value representable (by fixed point value 0) </li>
<li>
encoding-max - maximum floating point value representable (by fixed point value 255) </li>
</ul>
</li>
</ul>
</li>
<li>
Algorithm <ol>
<li>
Compute the true range (min, max) of input data. </li>
<li>
Compute the encoding-min and encoding-max. </li>
<li>
Quantize the input floating point values. </li>
<li>
Output: <ul>
<li>
fixed point values </li>
<li>
encoding-min and encoding-max parameters </li>
</ul>
</li>
</ol>
</li>
</ul>
<h2><a class="anchor" id="quantize_algorithm_details"></a>
Details</h2>
<ol>
<li>
<b>Compute the true range of the input floating point data.</b> <ul>
<li>
finds the smallest and largest values in the input data </li>
<li>
represents the true range of the input data</li>
</ul>
</li>
<li>
<b>Compute the encoding-min and encoding-max.</b> <ul>
<li>
These parameters are used in the quantization step. </li>
<li>
These parameters define the range and floating point values that will be representable by the fixed point format. <ul>
<li>
encoding-min: specifies the smallest floating point value that will be represented by the fixed point value of 0 </li>
<li>
encoding-max: specifies the largest floating point value that will be represented by the fixed point value of 255 </li>
<li>
floating point values at every step size, where step size = (encoding-max - encoding-min) / 255, will be representable </li>
</ul>
</li>
</ul>
<ol>
<li>
encoding-min and encoding-max are first set to the true min and true max computed in the previous step </li>
<li>
First requirement: encoding range must be at least a minimum of 0.01 <ul>
<li>
encoding-max is adjusted to max(true max, true min + 0.01) </li>
</ul>
</li>
<li>
Second requirement: floating point value of 0 must be exactly representable <ul>
<li>
encoding-min or encoding-max may be further adjusted </li>
</ul>
</li>
</ol>
</li>
<li>
<b>Handling 0.</b> <ol>
<li>
Case 1: Inputs are strictly positive <ul>
<li>
the encoding-min is set to 0.0 </li>
<li>
zero floating point value is exactly representable by smallest fixed point value 0 </li>
<li>
e.g. input range = [5.0, 10.0] <ul>
<li>
encoding-min = 0.0, encoding-max = 10.0 </li>
</ul>
</li>
</ul>
</li>
<li>
Case 2: Inputs are strictly negative <ul>
<li>
encoding-max is set to 0.0 </li>
<li>
zero floating point value is exactly representable by the largest fixed point value 255 </li>
<li>
e.g. input range = [-20.0, -6.0] <ul>
<li>
encoding-min = -20.0, encoding-max = 0.0 </li>
</ul>
</li>
</ul>
</li>
<li>
Case 3: Inputs are both negative and positive <ul>
<li>
encoding-min and encoding-max are slightly shifted to make the floating point zero exactly representable </li>
<li>
e.g. input range = [-5.1, 5.1] <ul>
<li>
encoding-min and encoding-max are first set to -5.1 and 5.1, respectively </li>
<li>
encoding range is 10.2 and the step size is 10.2/255 = 0.04 </li>
<li>
zero value is currently not representable. The closest values representable are -0.02 and +0.02 by fixed point values 127 and 128, respectively </li>
<li>
encoding-min and encoding-max are shifted by -0.02. The new encoding-min is -5.12 and the new encoding-max is 5.08 </li>
<li>
floating point zero is now exactly representable by the fixed point value of 128 </li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>
<b>Quantize the input floating point values.</b> <ul>
<li>
encoding-min and encoding-max parameter determined in the previous step are used to quantize all the input floating values to their fixed point representation </li>
<li>
Quantization formula is: <ul>
<li>
quantized value = round(255 * (floating point value - encoding.min) / (encoding.max - encoding.min)) </li>
</ul>
</li>
<li>
quantized value is also clamped to be within 0 and 255 </li>
</ul>
</li>
<li>
<b>Outputs</b> <ul>
<li>
the fixed point values </li>
<li>
encoding-min and encoding-max parameters </li>
</ul>
</li>
</ol>
<h2><a class="anchor" id="quantize_algorithm_examples"></a>
Quantization Example</h2>
<ul>
<li>
Inputs: <ul>
<li>
input values = [-1.8, -1.0, 0, 0.5] </li>
</ul>
</li>
<li>
encoding-min is set to -1.8 and encoding-max to 0.5 </li>
<li>
encoding range is 2.3, which is larger than the required 0.01 </li>
<li>
encoding-min is adjusted to −1.803922 and encoding-max to 0.496078 to make zero exactly representable </li>
<li>
step size is 0.009020 </li>
<li>
Outputs: <ul>
<li>
quantized values are [0, 89, 200, 255] </li>
</ul>
</li>
</ul>
<h2><a class="anchor" id="dequantize_algorithm_examples"></a>
Dequantization Example</h2>
<ul>
<li>
Inputs: <ul>
<li>
quantized values = [0, 89, 200, 255] </li>
<li>
encoding-min = −1.803922, encoding-max = 0.496078 </li>
</ul>
</li>
<li>
step size is 0.009020 </li>
<li>
Outputs: <ul>
<li>
dequantized values = [−1.8039, −1.0011, 0.0000, 0.4961] </li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="quantization_modes"></a>
Quantization Modes</h1>
<p>SNPE supports multiple quantization modes. All modes quantize parameters to 8-bit fixed point, the difference is in how quantization parameters are chosen.</p>
<h2><a class="anchor" id="default_quantization_mode"></a>
Default Quantization Mode</h2>
<p>The default mode has been described above, and uses the true min/max of the data being quantized, followed by an adjustment of the range to ensure a minimum range and to ensure 0.0 is exactly quantizable.</p>
<h2><a class="anchor" id="enhanced_quantization_mode"></a>
Enhanced Quantization Mode</h2>
<p>Enhanced quantization mode (invoked by using the "use_enhanced_quantizer" parameter to <a class="el" href="tools.html#tools_snpe-dlc-quantize">snpe-dlc-quantize</a>) uses an algorithm to try to determine a better set of quantization parameters to improve accuracy. The algorithm may pick a different min/max value than the default quantizer, and in some cases it may set the range such that some of the original weights and/or activations cannot fall into that range. However, this range does produce better accuracy than simply using the true min/max.</p>
<p>This is useful for some models where the weights and/or activations may have "long tails". (Imagine a range with most values between -100 and 1000, but a few values much greater than 1000 or much less than -100.) In some cases these long tails can be ignored and the range -100, 1000 can be used more effectively than the full range.</p>
<p>Enhanced quantizer still enforces a minimum range and ensures 0.0 is exactly quantizable.</p>
<h1><a class="anchor" id="quantization_impacts"></a>
Quantization Impacts</h1>
<p>Quantizing a model and/or running it in a quantized runtime (like the DSP) can affect accuracy. Some models may not work well when quantized, and may yield incorrect results. The metrics for measuring impact of quantization on a model that does classification are typically "Mean Average Precision", "Top-1 Error" and "Top-5 Error". These metrics published in SNPE release notes for various models. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
