<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
-->
<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"></meta>
<meta http-equiv="X-UA-Compatible" content="IE=9"></meta>
<title>Snapdragon Neural Processing Engine SDK: Performance Tips</title>
<link href="tabs.css" rel="stylesheet" type="text/css"></link>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="autoEnterCurrentDate.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="is.css" rel="stylesheet" type="text/css" ></link>
<link href="custom.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Snapdragon Neural Processing Engine SDK
   <span id="projectnumber"></span></div>
   <div id="projectbrief">Reference Guide</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('prog_performance.html','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Performance Tips </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="prog_performance_tensors"></a>
Performance Tips for Using Tensors</h1>
<h2><a class="anchor" id="user_buffer"></a>
UserBuffer</h2>
<p>By default SNPE creates networks that accept tensors, where for each SNPE::execute() there is an additional copy to get data into/out of SNPE. In addition, depending on the data format required by the underlying target runtime, SNPE may perform format conversion such as quantization or float expansion.</p>
<p>An alternative is to create networks that accept user buffers, by calling build with SNPEBuilder::build() with the setUseUserSuppliedBuffers() setter. This creates networks that will use UserBuffers for execute(). By utilizing UserBuffer, a user can specify the format (encoding) of the buffer and its dimensionality. If the dimensions and stride of the buffer matches the network's, SNPE can potentialy read from and write to the buffers directly, saving data copies into / out of tensors for each execute.</p>
<h2><a class="anchor" id="copy_tensor"></a>
Copy Tensors</h2>
<p>SNPE supports a STL compatible tensor class that is used to send data into the network and return the output. While this provides a great deal of flexibility and ability to leverage STL functions to manipulate the data, it does come at a cost. For tensors that contain relatively little data, exactly how the user manipulates the data inside a tensor or gets data into the tensor doesnâ€™t really matter. However, for tensors that need to contain a large amount of data (e.g. a 1080p input image or very large outputs), the user should be aware of the following guideline when moving data into a tensor: std::copy() is far more efficient for moving data into or out of a tensor than direct usage of the iterators (by at least an order of magnitude more). So rather than doing something like the following:</p>
<pre class="fragment">// Assume we have access to the following two variables
// std::shared_ptr&lt;zdl::DlSystem::ITensor&gt; tensor;
// std::vector&lt;float&gt;&amp; vec;
vec.resize(tensor-&gt;getSize());
size_t idx = 0;
for (auto it = tensor-&gt;begin(); it != tensor-&gt;end(); it++)
{
        vec[idx++] = *it;
}
</pre><p>The user should do this instead: </p><pre class="fragment">std::copy(tensor-&gt;begin(), tensor-&gt;end(), vec.begin())
</pre><p>This is true whether getting data from a tensor (as in the example above) or putting data into a tensor.</p>
<p>In addition, if the tensor data needs to be modified (e.g. pre-processed before going into the network or post-processed after), it is better to do the manipulation in a user buffer than in the tensor directly using the iterators (and then just use std::copy() to move the modified data in/out of the tensor).</p>
<h1><a class="anchor" id="prog_performance2"></a>
Performance Tips for Executing Networks</h1>
<ul>
<li>
<p class="startli"><b>Optimizing TensorFlow Graphs for Inference</b> </p><ul>
<li>
This applies only for TensorFlow, not Caffe. </li>
<li>
TensorFlow provides a tool that can be used to convert a model into one that is optimized for inference. </li>
<li>
It is <b>strongly</b> recommended to optimize TensorFlow graphs prior to converting them to a DLC file. </li>
<li>
For an example of optimizing for inference, see $SNPE_ROOT/models/inception_v3/scripts/setup_inceptionv3.py. </li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Balancing Performance and Power</b> </p><ul>
<li>
SNPE supports five performance profiles, "DEFAULT", "BALANCED, "HIGH_PERFORMANCE", "POWER_SAVER" and "SYSTEM_SETTINGS". (See setPerformanceProfile API description.) </li>
<li>
The DEFAULT performance profile is less power intensive, at the expense of performance. </li>
<li>
The BALANCED performance profile is the same as DEFAULT. (DEFAULT is going to be deprecated.) </li>
<li>
The POWER_SAVER performance profile attempts to provide more power saving than the BALANCED performance profile, which may result in lower performance. </li>
<li>
For optimal performance, use the setPerformanceProfile API to select HIGH_PERFORMANCE. <ul>
<li>
When HIGH_PERFORMANCE is selected, SNPE will attempt to maximize performance at the expense of increased power consumption. </li>
</ul>
</li>
<li>
The SYSTEM_SETTINGS profile causes SNPE to leave all power and performance settings alone. No calls to any power or performance related APIs will be invoked by SNPE. <ul>
<li>
Users of this profile can use other APIs (out of the scope of SNPE) if they want to control performance or power. </li>
</ul>
</li>
</ul>
<p class="endli"></p>
</li>
<li>
<p class="startli"><b>Running on the GPU</b> </p><ul>
<li>
Typically, running a network on the GPU results in a 6X-10X speed of inference increase as compared to running the same network on the CPU and at lower power consumption, so usually the GPU runtime is the obvious choice for network execution unless the GPU is potentially heavily utilized for some other application (e.g. gaming). </li>
<li>
However, there is a roughly 4-6ms overhead for network execution on the GPU that does not exist on the CPU, so very small networks might execute quicker on the CPU. For example, if a network runs in less than 10ms on the GPU, it may run faster on the CPU as the GPU overhead might eliminate any speed advantage to the actual network execution that the GPU provides. </li>
<li>
By default, the GPU runtime runs in GPU_FLOAT32_16_HYBRID mode (Please see <a href="group__c__plus__plus__apis.html">C++ Runtime_t Enum</a> description). The GPU_FLOAT16 mode may run some networks faster but may incur accuracy loss as well. (Please see <a href="limitations.html#limitations_gpu">GPU Limitations</a> section for more info.) </li>
</ul>
<p class="endli"></p>
</li>
<li>
<b>Running on the DSP</b> <ul>
<li>
The DSP offers an optimized execution environment for supported layers, however some layer operations are not optimal on the DSP and may cause slow execution of the model on the DSP. </li>
<li>
The performance of input preprocessing layers are currently not optimized on the DSP runtime. When using the DSP runtime it is recommended to do input preprocessing (colour space conversion, scaling, crop and mean subtract) before passing the image to SNPE. </li>
<li>
The DSP runs 8-bit quantized math for most operations. Some networks may be sensitive to this and may not be suitable for the DSP runtime. </li>
<li>
The default DSP runtime availability check performs platform validation on the DSP to validate DSP runtime support. Basic runtime availability check performs less validation than the default check, i.e. basic check only validates that the SoC platform should have DSP support. </li>
</ul>
</li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  Copyright (c) 2016-2018 Qualcomm Technologies, Inc.
  All Rights Reserved.
  Confidential and Proprietary - Qualcomm Technologies, Inc.
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 -->
<!-- start footer part -->
<div id="nav-path" class="navpath" font-size:small;><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">
      <p align="right">
        80-NL315-14 A <br>
        MAY CONTAIN U.S. AND INTERNATIONAL EXPORT CONTROLLED INFORMATION
        <!--If the Controlled Distribution statement is to be included, uncomment below:-->
        <!--<b>Controlled Distribution - DO NOT COPY</b>-->
        <img class="footer" width:5%; alt="QTI Logo" src="images/QTI_Logo.png" />
      </p>
    </li>
  </ul>
</div>
</body>
</html>
